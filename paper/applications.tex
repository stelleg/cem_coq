\section{Applications}

Now that we've seen how the compiler preserves the semantics of the source
language, we can look at what sorts of tools this gives us for reasoning about
the output of the compiler. This includes properties like runtime, heap usage,
stack size, types, and of course, correctness of the results.

\subsection{Time}

One of the most common concerns for someone writing source code for a compiler
is runtime. The ability of programmers to reason about their code's performance
is hampered by the common philosophy of compilers: use any means necessary, no
matter how hard to reason about at the source level, to make code run fast. This
is a reasonable default; when reasoning about code performance, it is a very
rare case that the programmer \emph{doesn't} want their code to run as fast as
possible. 

That said, there are cases when the compiler isn't clever enough. The long-lived
hope for the \emph{sufficiently clever compiler} is dead. Instead, programmers
are forced to rewrite their code to improve performance. Re-writing code to
improve performance will often require reasoning about opaque decisions made by
an optimizing compiler; not an easy task by any measure.

For example, a canonical example of beautiful Haskell code is the efficient
implementation of the fibonacci sequence: 

\begin{lstlisting}[language=Haskell]
fibs = 1:1:zipWith (+) fibs (tail fibs)
\end{lstlisting}

Because Haskell is technically a non-strict language, we rely on our compiler to
"make the right choice" and implement the above \texttt{fibs} using
call-by-need, as using call-by-name would result in exponential complexity.
Relying on opaque decisions like this, when the difference is
\emph{exponential}, is problematic, and makes reasoning about code performance
in a reliable way impossible. We argue that with a verified compiler with a
fixed semantics we improve the programmers ability to reason formally about
performance. 

Towards this goal, we present a theorem allowing the programmer to reason
formally about the time, in number of instructions, that the computation will
take. Of course, number of instructions is a mediocre-at-best approximation of
time, but it will at least prevent any asymptotic surprises, like the
\texttt{fibs} case above. To do this

\subsection{Types}

When we discuss reasoning about our programs at the source level, it would be
irresponsible to give types anything less than their own subsection. Types are
how we do formal reasoning about programs in practice, and when building a
compiler, we'd like to make sure this reasoning is preserved to machine code.
Chlipala showed elegantly how types can be provably preserved through
compilation of simply typed strict lambda calculus in \cite{?}. While this paper
has focused on untyped lambda calculus, we use this section to show how one
could extend this work to show that type-based reasoning will be preserved
through evaluation.  

Unfortunately, because we've already fixed our term syntax to be untyped, we
cannot do this reasoning within the common context of explicitly typed
langauges, e.g. System F or the Calculus of Constructions. Instead, we must turn
to a completely implicit type system, which makes type judgements on simple
lambda terms, not unlike a subset of existing Hindley-Milner style languages. We
see no reasoning that this line of reasoning should not extend to more powerful
explicit type systems. 

To formalize this, we generalize the notion of a type judgement to any relation
between a source term and some arbtrary type, represented as a Type variable in
Coq. Leaving the type system completely abstract, we define a typing judgement
to be of the form:

\begin{lstlisting}
Variable type : Type.
Variable hasType : relation tm type.
\end{lstlisting}

To reason about our abstract type we turn to a property that just about every
type system must ahere to: preservation. Preservation says that if a term has a
type, and takes one or more steps, its type must be preserved. We choose to
define preservation using the semantics of call-by-name, as they are our
simplest semantics when to use when we aren't concerned with operational
subtleties.

\begin{lstlisting}
Variable preservation : ∀ (t t':tm) (tau:type), 
  hasType t tau → 
  cbname.step t t' →
  hasType t' tau.
\end{lstlisting}

Given a proof of presevation for our type system for call-by-name, we can prove
that the compiled code respects this, similar to Chlipala's proof that his
simply typed lambda calculus compiler was type-preserving \cite{?}.

\begin{lstlisting}
Theorem assembly_preserves : ∀ t tau p s v, 
  hasType t tau → 
  compiles t p → 
  executes p s →
  term_machine_state_rel s v → 
  hasType v tau.
\end{lstlisting}

In words, this means that if our source term has type \texttt{tau} and
succesfully compiles (likely that typechecking would imply succesful compilation,
but we don't require it), and the compiled program \texttt{p} evaluates to some
machine state \texttt{s}, then any term related by our correctness relation will
also have type \texttt{tau}.

\subsection{Space}

Reasoning about space usage in lazy languages is notoriously difficult.
Optimization decisions like deforestation can have effects on the
\emph{asymtotic} memory consumption, making attempts to formally bound memory
consumption impractical at best. 

Generally there are two ways space computations can fail: the one we will focus
on currently is stack overflow. Stack overflows occur when the maximum allocated
stack space is exceeded. This kind of failure is ubiquitous and hard to reason
about. It is the cited cause of failures and even deaths in the case of Toyota's
unintended acceleration case \cite{?}. We show how reasoning about stack size at
the source semantics can be preserved through to machine code, potentially
preventing this kind of failure.  

Reasoning about heap space is a bit more difficult currently. This is
because we build on the call-by-need semantics that ensures variables are fresh
with respect to the \emph{entire} heap domain. There are a few ways around this.
One would be to incorporate the notion of a garbage collector as an intermediate
step between reduction steps. A more elegant approach, and that taken by
Chlipala~\cite{chlipala}, is to relax the freshness constraint to be only fresh
with respect to \emph{reachable} heap locations. In this way, we get a more
direct way of reasoning about space usage. With some tools to reason about space
usage at the source level, we could then ensure that this class of reasoning
about heap space is preserved through to the machine code.

\subsection{Programs that \emph{Really} Can't Go Wrong}

Typed functional language programmers often tout on the fact that their programs
"can't go wrong". This important property of typed functional languages is
hampered by an asterisk: "unless it runs out of memory".  The previous sections
on formal reasoning about time and space requirements using this verified
compiler provide an exciting area for future research: removing this asterisk.
In this way, we take a step towards programs that \emph{really} don't go wrong;
they provably won't run out of space or time. This is incredibly useful for many
domains, where these resources are constrained, but we would still like
correctness properties to hold. For example, in the Toyota case above, a proof
that the stack stayed within its bounds could have potentially even saved lives.
This approach also allows us to specify bounds on number of instructions, as
there are many cases in which the number of instructions before a computation
completes is strictly bounded, e.g. heart monitors, video games, etc. A
microcontroller in a car that has a proof that "when it terminates, it will do
the right thing", is of little use.  

Essentially, this ability to broaden the sense in which programs "can't go
wrong", and prove that these properties are preserved through compilation is an
exciting area that this paper takes a step towards. Of course, there is still
much work to be done, as reasoning about these properties even at the source
level is incredibly difficult.


